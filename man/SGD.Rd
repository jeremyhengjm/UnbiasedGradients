% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SGD.R
\name{SGD}
\alias{SGD}
\title{Stochastic gradient descent}
\usage{
SGD(
  model,
  theta_initial,
  observations,
  nparticles,
  resampling_threshold = 1,
  coupled2_resampling,
  coupled4_resampling,
  k = 0,
  m = 1,
  minimum_level,
  maximum_level,
  level_distribution,
  learning_rate = 0.001,
  stopping_threshold = 1e-04,
  max_iterations = 1e+06,
  mcmc_iter = 0
)
}
\arguments{
\item{model}{a list representing a hidden Markov model, e.g. \code{\link{hmm_ornstein_uhlenbeck}}}

\item{theta_initial}{an initial vector of parameters}

\item{observations}{a matrix of observations of size nobservations x ydimension}

\item{nparticles}{number of particles}

\item{resampling_threshold}{ESS proportion below which resampling is triggered (always resample at observation times by default)}

\item{coupled2_resampling}{a 2-way coupled resampling scheme, such as \code{\link{coupled2_maximal_coupled_residuals}}}

\item{coupled4_resampling}{a 4-way coupled resampling scheme, such as \code{\link{coupled4_maximal_coupled_residuals}}}

\item{k}{iteration at which to start averaging (default to 0)}

\item{m}{iteration at which to stop averaging (default to 1)}

\item{minimum_level}{coarsest discretization level}

\item{maximum_level}{finest discretization level}

\item{level_distribution}{list containing mass_function and tail_function that specify the distribution of levels, 
e.g. by calling \code{\link{compute_level_distribution}}}

\item{learning_rate}{stepsize of the SGD algorithm}

\item{stopping_threshold}{criterion to terminate iterations}

\item{max_iterations}{maximum number of SGD iterations}

\item{mcmc_iter}{use unbiased estimate if mcmc_iter == 0 or mcmc estimate at maximum_level with mcmc_iter iterations if mcmc_iter > 0}
}
\value{
a list with objects such as 
theta parameters at the last SGD iteration
trajectory parameters across the SGD iterations
}
\description{
Run a stochastic gradient descent using unbiased estimator of the gradient of the log-likelihood
}
